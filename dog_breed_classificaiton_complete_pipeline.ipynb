{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 5778981,
          "sourceType": "datasetVersion",
          "datasetId": 3320287
        }
      ],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "efficientnetv2b2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devashish-Mishra-2003/dog_classifier_notebook/blob/main/efficientnetv2b2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "vikaschauhan734_120_dog_breed_image_classification_path = kagglehub.dataset_download('vikaschauhan734/120-dog-breed-image-classification')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "cvn3Jgn0TbLZ"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook solves dog breed classification problem by using EfficientNetV2B2\n",
        "\n",
        "Max Val_Accuracy : 89.978\n",
        "Best Predict on Test : 89.72\n",
        "\n",
        "Author : Devashish Mishra"
      ],
      "metadata": {
        "id": "chmc-rBaTbLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Kaggle-only config and strategy\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# reproducibility\n",
        "SEED = 777\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# ---------- Kaggle dataset detection ----------\n",
        "KAGGLE_INPUT_ROOT = \"/kaggle/input\"\n",
        "CANDIDATE_DS = [\"120-dog-breed-image-classification\"]\n",
        "\n",
        "dataset_root = None\n",
        "for d in CANDIDATE_DS:\n",
        "    path = os.path.join(KAGGLE_INPUT_ROOT, d)\n",
        "    if os.path.exists(path):\n",
        "        dataset_root = path\n",
        "        break\n",
        "if dataset_root is None:\n",
        "    entries = sorted(os.listdir(KAGGLE_INPUT_ROOT))\n",
        "    if len(entries) == 0:\n",
        "        raise FileNotFoundError(\"No datasets under /kaggle/input.\")\n",
        "    dataset_root = os.path.join(KAGGLE_INPUT_ROOT, entries[0])\n",
        "\n",
        "# prefer an 'images' subfolder if present\n",
        "if os.path.isdir(os.path.join(dataset_root, \"images\")):\n",
        "    SRC_ROOT = os.path.join(dataset_root, \"images\")\n",
        "else:\n",
        "    SRC_ROOT = dataset_root\n",
        "\n",
        "# writable processed output\n",
        "PROCESSED_ROOT = \"/kaggle/working/processed_dogs\"\n",
        "\n",
        "# outputs\n",
        "MODEL_DIR = Path(\"/kaggle/working/models\"); MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "BEST_PHASEA = MODEL_DIR / \"best_phaseA.keras\"\n",
        "BEST_PHASEB = MODEL_DIR / \"best_phaseB.keras\"\n",
        "COMPACT_FINAL = MODEL_DIR / \"final_compact.keras\"\n",
        "SAVEDMODEL_DIR = MODEL_DIR / \"savedmodel_backup\"\n",
        "TFLITE_OUT = MODEL_DIR / \"final_fp16.tflite\"\n",
        "\n",
        "# image / training config\n",
        "IMG_SIZE = (300, 300)\n",
        "ALLOWED_EXT = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
        "TRAIN_RATIO, VAL_RATIO, TEST_RATIO = 0.70, 0.20, 0.10\n",
        "\n",
        "# single GPU strategy\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "for g in gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "strategy = tf.distribute.get_strategy()\n",
        "\n",
        "# batch size safe for P100\n",
        "GLOBAL_BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# two-phase hyperparams\n",
        "EPOCHS_HEAD = 10\n",
        "LR_HEAD = 1e-5    # lower learning rate for head\n",
        "EPOCHS_FINE = 20\n",
        "LR_FINE = 1e-6    # safer fine-tuning\n",
        "UNFREEZE_TOP_N = 100\n",
        "\n",
        "\n",
        "# print summary\n",
        "print(\"Kaggle dataset root:\", dataset_root)\n",
        "print(\"SRC_ROOT:\", SRC_ROOT)\n",
        "print(\"PROCESSED_ROOT:\", PROCESSED_ROOT)\n",
        "print(\"Strategy:\", type(strategy).__name__)\n",
        "print(\"GLOBAL_BATCH_SIZE:\", GLOBAL_BATCH_SIZE)\n",
        "print(\"IMG_SIZE:\", IMG_SIZE)\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-24T14:43:07.949131Z",
          "iopub.execute_input": "2025-09-24T14:43:07.949434Z",
          "iopub.status.idle": "2025-09-24T14:43:07.959874Z",
          "shell.execute_reply.started": "2025-09-24T14:43:07.949382Z",
          "shell.execute_reply": "2025-09-24T14:43:07.959093Z"
        },
        "id": "fJwAJSzZTbLc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Preprocess (resizes + splits per class into train/val/test)\n",
        "import os, shutil, json\n",
        "from pathlib import Path\n",
        "import random\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "def ensure_dir(p): os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def collect_classes(src_root):\n",
        "    src = Path(src_root)\n",
        "    if not src.exists():\n",
        "        raise FileNotFoundError(f\"SRC_ROOT not found: {src_root}\")\n",
        "    classes = {}\n",
        "    for d in sorted(os.listdir(src_root)):\n",
        "        p = src / d\n",
        "        if p.is_dir():\n",
        "            files = [str(p / f) for f in sorted(os.listdir(p))\n",
        "                     if os.path.splitext(f.lower())[1] in ALLOWED_EXT]\n",
        "            if files:\n",
        "                classes[d] = files\n",
        "    return classes\n",
        "\n",
        "def resize_image_cv2(path, size):\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        raise IOError(f\"Failed to read image: {path}\")\n",
        "    return cv2.resize(img, (size[0], size[1]), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def save_jpg(img_bgr, out_path, quality=95):\n",
        "    ensure_dir(os.path.dirname(out_path))\n",
        "    cv2.imwrite(out_path, img_bgr, [int(cv2.IMWRITE_JPEG_QUALITY), quality])\n",
        "\n",
        "def split_per_class(files, train_ratio=TRAIN_RATIO, val_ratio=VAL_RATIO, test_ratio=TEST_RATIO):\n",
        "    f = files.copy()\n",
        "    random.shuffle(f)\n",
        "    n = len(f)\n",
        "    if n == 0:\n",
        "        return [], [], []\n",
        "    train_n = int(round(n * train_ratio))\n",
        "    val_n = int(round(n * val_ratio))\n",
        "    if train_n == 0 and n >= 1:\n",
        "        train_n = 1\n",
        "    test_n = n - train_n - val_n\n",
        "    if test_n < 0:\n",
        "        val_n += test_n\n",
        "        test_n = 0\n",
        "    if val_n == 0 and n - train_n > 0:\n",
        "        val_n = 1\n",
        "        test_n = n - train_n - val_n\n",
        "    if test_n == 0 and n - train_n - val_n > 0:\n",
        "        test_n = n - train_n - val_n\n",
        "    if train_n + val_n + test_n != n:\n",
        "        remainder = n - (train_n + val_n + test_n)\n",
        "        train_n += remainder\n",
        "    assert train_n + val_n + test_n == n\n",
        "    return f[:train_n], f[train_n:train_n+val_n], f[train_n+val_n:]\n",
        "\n",
        "def preprocess_dataset(src_root, out_root, img_size=IMG_SIZE):\n",
        "    classes = collect_classes(src_root)\n",
        "    class_names = sorted(classes.keys())\n",
        "    print(f\"Found {len(class_names)} classes.\")\n",
        "    if os.path.exists(out_root):\n",
        "        print(\"Removing existing processed folder:\", out_root)\n",
        "        shutil.rmtree(out_root)\n",
        "    for s in (\"train\",\"val\",\"test\"):\n",
        "        ensure_dir(os.path.join(out_root, s))\n",
        "\n",
        "    class_indices = {i: name for i,name in enumerate(class_names)}\n",
        "    ensure_dir(out_root)\n",
        "    with open(os.path.join(out_root, \"class_indices.json\"), \"w\") as f:\n",
        "        json.dump(class_indices, f, indent=2)\n",
        "\n",
        "    train_recs, val_recs, test_recs = [], [], []\n",
        "    for cls in tqdm(class_names, desc=\"Classes\"):\n",
        "        files = classes[cls]\n",
        "        tr, va, te = split_per_class(files)\n",
        "        def save_list(lst, split_name, recs):\n",
        "            out_dir_label = os.path.join(out_root, split_name, cls)\n",
        "            ensure_dir(out_dir_label)\n",
        "            for src in lst:\n",
        "                try:\n",
        "                    img = resize_image_cv2(src, img_size)\n",
        "                except Exception as e:\n",
        "                    print(\"SKIP read error:\", src, e)\n",
        "                    continue\n",
        "                base = Path(src).stem\n",
        "                out_jpg = os.path.join(out_dir_label, base + \".jpg\")\n",
        "                save_jpg(img, out_jpg)\n",
        "                recs.append({\"orig_path\": src, \"out_jpg\": out_jpg, \"label\": cls})\n",
        "        save_list(tr, \"train\", train_recs)\n",
        "        save_list(va, \"val\", val_recs)\n",
        "        save_list(te, \"test\", test_recs)\n",
        "\n",
        "    pd.DataFrame(train_recs).to_csv(os.path.join(out_root, \"manifest_train.csv\"), index=False)\n",
        "    pd.DataFrame(val_recs).to_csv(os.path.join(out_root, \"manifest_val.csv\"), index=False)\n",
        "    pd.DataFrame(test_recs).to_csv(os.path.join(out_root, \"manifest_test.csv\"), index=False)\n",
        "\n",
        "    # summary counts\n",
        "    def counts_from_dir(split_dir):\n",
        "        counts = {}\n",
        "        for c in class_names:\n",
        "            cls_dir = os.path.join(split_dir, c)\n",
        "            n = len([f for f in os.listdir(cls_dir) if os.path.splitext(f.lower())[1] in ALLOWED_EXT]) if os.path.isdir(cls_dir) else 0\n",
        "            counts[c] = n\n",
        "        return counts\n",
        "\n",
        "    counts = {\"train\": counts_from_dir(os.path.join(out_root,\"train\")),\n",
        "              \"val\": counts_from_dir(os.path.join(out_root,\"val\")),\n",
        "              \"test\": counts_from_dir(os.path.join(out_root,\"test\"))}\n",
        "    totals = {k: sum(v.values()) for k,v in counts.items()}\n",
        "    with open(os.path.join(out_root, \"counts_summary.json\"), \"w\") as f:\n",
        "        json.dump({\"counts\":counts,\"totals\":totals}, f, indent=2)\n",
        "\n",
        "    print(\"Done. Totals:\", totals)\n",
        "    return out_root\n",
        "\n",
        "# Run preprocessing if necessary\n",
        "if not Path(PROCESSED_ROOT).exists():\n",
        "    print(\"Creating processed dataset at:\", PROCESSED_ROOT)\n",
        "    preprocess_dataset(SRC_ROOT, PROCESSED_ROOT, img_size=IMG_SIZE)\n",
        "else:\n",
        "    print(\"Found existing processed dataset at:\", PROCESSED_ROOT)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-24T14:32:59.72763Z",
          "iopub.execute_input": "2025-09-24T14:32:59.728136Z",
          "iopub.status.idle": "2025-09-24T14:37:58.944192Z",
          "shell.execute_reply.started": "2025-09-24T14:32:59.728116Z",
          "shell.execute_reply": "2025-09-24T14:37:58.94343Z"
        },
        "id": "_LfnWQl8TbLd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: build tf.data datasets\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "\n",
        "train_dir = str(Path(PROCESSED_ROOT) / \"train\")\n",
        "val_dir   = str(Path(PROCESSED_ROOT) / \"val\")\n",
        "test_dir  = str(Path(PROCESSED_ROOT) / \"test\")\n",
        "\n",
        "print(\"train_dir:\", train_dir)\n",
        "print(\"val_dir:\", val_dir)\n",
        "print(\"test_dir:\", test_dir)\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=GLOBAL_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=GLOBAL_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=GLOBAL_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "NUM_CLASSES = len(train_ds.class_names)\n",
        "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
        "\n",
        "# performance tuning\n",
        "train_ds = train_ds.prefetch(AUTOTUNE)\n",
        "val_ds   = val_ds.prefetch(AUTOTUNE)\n",
        "test_ds  = test_ds.prefetch(AUTOTUNE)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-24T14:37:58.945787Z",
          "iopub.execute_input": "2025-09-24T14:37:58.945977Z",
          "iopub.status.idle": "2025-09-24T14:38:01.571771Z",
          "shell.execute_reply.started": "2025-09-24T14:37:58.945961Z",
          "shell.execute_reply": "2025-09-24T14:38:01.571209Z"
        },
        "id": "yEpiV_iSTbLe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: build model (EfficientNetV2B2 + stronger head + preprocess_input)\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2B2, preprocess_input\n",
        "\n",
        "with strategy.scope():\n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.2),\n",
        "        layers.RandomZoom(0.2),\n",
        "        layers.RandomContrast(0.1),\n",
        "    ], name=\"augmentation\")\n",
        "\n",
        "    base = EfficientNetV2B2(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
        "    )\n",
        "    base.trainable = False\n",
        "\n",
        "    inputs = layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "    x = data_augmentation(inputs)\n",
        "    x = preprocess_input(x)         # ✅ official preprocessing\n",
        "    x = base(x, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(1024, activation=\"relu\", dtype=\"float32\")(x)\n",
        "    x = layers.Dropout(0.5)(x)      # stronger regularization\n",
        "    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\", dtype=\"float32\")(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=LR_HEAD),\n",
        "                  loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-24T14:43:40.058947Z",
          "iopub.execute_input": "2025-09-24T14:43:40.059509Z",
          "iopub.status.idle": "2025-09-24T14:43:41.957147Z",
          "shell.execute_reply.started": "2025-09-24T14:43:40.059483Z",
          "shell.execute_reply": "2025-09-24T14:43:41.956439Z"
        },
        "id": "fHIQNdKkTbLe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: callbacks\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "checkpoint_a = ModelCheckpoint(str(BEST_PHASEA), monitor=\"val_accuracy\", save_best_only=True, verbose=1)\n",
        "reducelr_a = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "early_a = EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True, verbose=1)\n",
        "\n",
        "checkpoint_b = ModelCheckpoint(str(BEST_PHASEB), monitor=\"val_accuracy\", save_best_only=True, verbose=1)\n",
        "reducelr_b = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "early_b = EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True, verbose=1)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-24T14:38:06.462622Z",
          "iopub.execute_input": "2025-09-24T14:38:06.462865Z",
          "iopub.status.idle": "2025-09-24T14:38:06.469414Z",
          "shell.execute_reply.started": "2025-09-24T14:38:06.462841Z",
          "shell.execute_reply": "2025-09-24T14:38:06.468957Z"
        },
        "id": "iSb1X5VoTbLf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: train Phase A (head-only)\n",
        "print(\"Phase A: training head only. LR =\", LR_HEAD)\n",
        "history_a = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS_HEAD,\n",
        "    callbacks=[checkpoint_a, reducelr_a, early_a],\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Phase A done. Best saved to:\", BEST_PHASEA)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-24T14:43:51.999269Z",
          "iopub.execute_input": "2025-09-24T14:43:51.999903Z",
          "iopub.status.idle": "2025-09-24T14:52:58.464121Z",
          "shell.execute_reply.started": "2025-09-24T14:43:51.999876Z",
          "shell.execute_reply": "2025-09-24T14:52:58.463433Z"
        },
        "id": "KSpJNDqVTbLf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: prepare Phase B (unfreeze top layers)\n",
        "# quick hyperparam tweaks for Phase B\n",
        "UNFREEZE_TOP_N = 150\n",
        "LR_FINE = 1e-5\n",
        "EPOCHS_FINE = 30\n",
        "\n",
        "# load best Phase A\n",
        "if Path(BEST_PHASEA).exists():\n",
        "    print(\"Loading best Phase A from:\", BEST_PHASEA)\n",
        "    model = tf.keras.models.load_model(str(BEST_PHASEA))\n",
        "else:\n",
        "    print(\"BEST_PHASEA not found; using current model in memory\")\n",
        "\n",
        "# unfreeze last UNFREEZE_TOP_N layers\n",
        "total_layers = len(model.layers)\n",
        "start_unfreeze = max(0, total_layers - UNFREEZE_TOP_N)\n",
        "for i, layer in enumerate(model.layers):\n",
        "    layer.trainable = True if i >= start_unfreeze else False\n",
        "print(f\"Unfreezing layers from {start_unfreeze} / {total_layers-1}\")\n",
        "\n",
        "# recompile with slightly higher LR and label smoothing\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "with strategy.scope():\n",
        "    opt = Adam(learning_rate=LR_FINE)\n",
        "    loss = CategoricalCrossentropy(label_smoothing=0.05)   # stabilizes loss\n",
        "    model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
        "\n",
        "# callbacks: you can keep the existing ones but increase patience a bit\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "checkpoint_b = ModelCheckpoint(str(BEST_PHASEB), monitor=\"val_accuracy\", save_best_only=True, verbose=1)\n",
        "reducelr_b = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "early_b = EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True, verbose=1)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-24T15:06:04.419386Z",
          "iopub.execute_input": "2025-09-24T15:06:04.419996Z",
          "iopub.status.idle": "2025-09-24T15:06:06.262086Z",
          "shell.execute_reply.started": "2025-09-24T15:06:04.419971Z",
          "shell.execute_reply": "2025-09-24T15:06:06.261307Z"
        },
        "id": "tQDPT6VHTbLg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: train Phase B (fine-tune)\n",
        "# resume training Phase B\n",
        "history_b = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS_FINE,\n",
        "    callbacks=[checkpoint_b, reducelr_b, early_b],\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Phase B done. Best saved to:\", BEST_PHASEB)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-24T15:06:18.003387Z",
          "iopub.execute_input": "2025-09-24T15:06:18.003702Z",
          "iopub.status.idle": "2025-09-24T16:35:46.91901Z",
          "shell.execute_reply.started": "2025-09-24T15:06:18.003683Z",
          "shell.execute_reply": "2025-09-24T16:35:46.918204Z"
        },
        "id": "tTK3TC8ATbLg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Option A: Continue fine-tuning, save final_model.keras, evaluate on test_ds\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# --- hyperparams (tweakable) ---\n",
        "UNFREEZE_TOP_N = 250        # last N layers to unfreeze\n",
        "LR_CONT = 3e-5              # small LR for continued fine-tuning\n",
        "EPOCHS_CONT = 20            # run a few to many epochs\n",
        "PATIENCE = 6                # early stopping patience\n",
        "FINAL_OUT = MODEL_DIR / \"final_model.keras\"   # where final model will be saved\n",
        "\n",
        "# --- load best Phase B checkpoint ---\n",
        "if not Path(BEST_PHASEB).exists():\n",
        "    raise FileNotFoundError(f\"BEST_PHASEB not found at {BEST_PHASEB}\")\n",
        "model = tf.keras.models.load_model(str(BEST_PHASEB))\n",
        "print(\"Loaded BEST_PHASEB:\", BEST_PHASEB)\n",
        "\n",
        "# --- unfreeze last UNFREEZE_TOP_N layers ---\n",
        "total_layers = len(model.layers)\n",
        "start_unfreeze = max(0, total_layers - UNFREEZE_TOP_N)\n",
        "for i, layer in enumerate(model.layers):\n",
        "    layer.trainable = True if i >= start_unfreeze else False\n",
        "print(f\"Unfrozen layers from index {start_unfreeze} / {total_layers-1}\")\n",
        "\n",
        "# --- recompile with small LR and label smoothing ---\n",
        "opt = Adam(learning_rate=LR_CONT)\n",
        "loss = CategoricalCrossentropy(label_smoothing=0.05)\n",
        "model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# --- callbacks (save best during this continuation) ---\n",
        "ckpt = ModelCheckpoint(str(MODEL_DIR / \"cont_finetune_best.keras\"),\n",
        "                       monitor=\"val_accuracy\", save_best_only=True, verbose=1)\n",
        "reducelr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "early = EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True, verbose=1)\n",
        "\n",
        "# --- train ---\n",
        "history_cont = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS_CONT,\n",
        "    callbacks=[ckpt, reducelr, early],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- after training: pick the best checkpoint (if exists) and save as final_model.keras ---\n",
        "best_cont_path = MODEL_DIR / \"cont_finetune_best.keras\"\n",
        "if best_cont_path.exists():\n",
        "    final = tf.keras.models.load_model(str(best_cont_path))\n",
        "    print(\"Loaded continuation best from:\", best_cont_path)\n",
        "else:\n",
        "    final = model\n",
        "    print(\"Using last-in-memory model as final.\")\n",
        "\n",
        "# save final model in native Keras format\n",
        "final.save(str(FINAL_OUT), include_optimizer=False)\n",
        "print(\"Saved final model ->\", FINAL_OUT)\n",
        "\n",
        "# --- evaluation on test_ds ---\n",
        "y_true, y_pred = [], []\n",
        "for imgs, labs in test_ds:\n",
        "    probs = final.predict(imgs, verbose=0)\n",
        "    y_true.extend(np.argmax(labs.numpy(), axis=1).tolist())\n",
        "    y_pred.extend(np.argmax(probs, axis=1).tolist())\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(f\"\\nFinal test accuracy: {acc:.4f}\")\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_true, y_pred, digits=4))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-24T16:56:20.489749Z",
          "iopub.execute_input": "2025-09-24T16:56:20.490302Z",
          "iopub.status.idle": "2025-09-24T17:44:56.955447Z",
          "shell.execute_reply.started": "2025-09-24T16:56:20.49028Z",
          "shell.execute_reply": "2025-09-24T17:44:56.954776Z"
        },
        "id": "udOHqXNgTbLg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: evaluation\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "if Path(BEST_PHASEB).exists():\n",
        "    final_model = tf.keras.models.load_model(str(BEST_PHASEB))\n",
        "    print(\"Loaded BEST_PHASEB for evaluation\")\n",
        "else:\n",
        "    final_model = model\n",
        "    print(\"Evaluating current model in memory\")\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for imgs, labs in test_ds:\n",
        "    probs = final_model.predict(imgs, verbose=0)\n",
        "    y_true.extend(np.argmax(labs.numpy(), axis=1).tolist())\n",
        "    y_pred.extend(np.argmax(probs, axis=1).tolist())\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(f\"Test accuracy: {acc:.4f}\")\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_true, y_pred, digits=4))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-24T17:45:32.963883Z",
          "iopub.execute_input": "2025-09-24T17:45:32.964157Z",
          "iopub.status.idle": "2025-09-24T17:45:58.903906Z",
          "shell.execute_reply.started": "2025-09-24T17:45:32.964135Z",
          "shell.execute_reply": "2025-09-24T17:45:58.903146Z"
        },
        "id": "WDneX42xTbLh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Progressive-resize (320x320) starting from best_phaseB.keras\n",
        "# Run this cell to fine-tune at 320x320 using BEST Phase B weights as the source.\n",
        "\n",
        "# Progressive-resize (320x320) starting from best_phaseB.keras\n",
        "# Corrected: keep raw datasets to read class_names before prefetch.\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2B2, preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ---------- params ----------\n",
        "NEW_IMG_SIZE = (320, 320)\n",
        "BATCH_SIZE_320 = 24           # try 24 on P100; reduce to 16 if OOM\n",
        "UNFREEZE_TOP_N = 250\n",
        "LR_RESIZE = 3e-6\n",
        "EPOCHS_RESIZE = 18\n",
        "PATIENCE = 5\n",
        "FINAL_RESIZED = MODEL_DIR / \"best_phaseB.keras\"\n",
        "\n",
        "SRC_TRAIN = str(Path(PROCESSED_ROOT) / \"train\")\n",
        "SRC_VAL   = str(Path(PROCESSED_ROOT) / \"val\")\n",
        "SRC_TEST  = str(Path(PROCESSED_ROOT) / \"test\")\n",
        "\n",
        "# ---------- sanity checks ----------\n",
        "assert Path(SRC_TRAIN).exists(), f\"Train folder not found: {SRC_TRAIN}\"\n",
        "assert Path(SRC_VAL).exists(), f\"Val folder not found: {SRC_VAL}\"\n",
        "assert Path(SRC_TEST).exists(), f\"Test folder not found: {SRC_TEST}\"\n",
        "best_phaseb_path = MODEL_DIR / \"best_phaseB.keras\"\n",
        "assert best_phaseb_path.exists(), f\"best_phaseB.keras not found at {best_phaseb_path}\"\n",
        "\n",
        "# ---------- datasets at 320 (keep raw dataset to access class_names/cardinality) ----------\n",
        "train_ds_320_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    SRC_TRAIN, labels='inferred', label_mode='categorical',\n",
        "    image_size=NEW_IMG_SIZE, batch_size=BATCH_SIZE_320, shuffle=True, seed=SEED\n",
        ")\n",
        "val_ds_320_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    SRC_VAL, labels='inferred', label_mode='categorical',\n",
        "    image_size=NEW_IMG_SIZE, batch_size=BATCH_SIZE_320, shuffle=False, seed=SEED\n",
        ")\n",
        "test_ds_320_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    SRC_TEST, labels='inferred', label_mode='categorical',\n",
        "    image_size=NEW_IMG_SIZE, batch_size=BATCH_SIZE_320, shuffle=False, seed=SEED\n",
        ")\n",
        "\n",
        "# read class names and batch counts from the raw datasets\n",
        "NUM_CLASSES = len(train_ds_320_raw.class_names)\n",
        "train_batches = tf.data.experimental.cardinality(train_ds_320_raw).numpy()\n",
        "print(\"NUM_CLASSES:\", NUM_CLASSES, \"Train batches:\", train_batches)\n",
        "\n",
        "# now safely add performance transforms\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds_320 = train_ds_320_raw.prefetch(AUTOTUNE)\n",
        "val_ds_320   = val_ds_320_raw.prefetch(AUTOTUNE)\n",
        "test_ds_320  = test_ds_320_raw.prefetch(AUTOTUNE)\n",
        "\n",
        "# ---------- build model (320) ----------\n",
        "inputs = layers.Input(shape=(NEW_IMG_SIZE[0], NEW_IMG_SIZE[1], 3))\n",
        "# mild augmentation\n",
        "x = layers.RandomFlip(\"horizontal\")(inputs)\n",
        "x = layers.RandomRotation(0.15)(x)\n",
        "x = layers.RandomZoom(0.12)(x)\n",
        "x = layers.RandomContrast(0.08)(x)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "base = EfficientNetV2B2(include_top=False, weights=None, input_shape=(NEW_IMG_SIZE[0], NEW_IMG_SIZE[1], 3))\n",
        "x = base(x, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(1024, activation=\"relu\", dtype=\"float32\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\", dtype=\"float32\")(x)\n",
        "model_320 = Model(inputs, outputs)\n",
        "\n",
        "# ---------- load weights from best_phaseB.keras ----------\n",
        "src_path = best_phaseb_path\n",
        "print(\"Loading weights from:\", src_path)\n",
        "src_model = tf.keras.models.load_model(str(src_path))\n",
        "\n",
        "# Try direct set_weights; fall back to per-layer matching if shapes differ\n",
        "try:\n",
        "    model_320.set_weights(src_model.get_weights())\n",
        "    print(\"Directly loaded weights from best_phaseB.keras\")\n",
        "except Exception as e:\n",
        "    print(\"Direct load failed (likely shape mismatch). Attempting best-effort layer-wise copy. Error:\", e)\n",
        "    copied = 0\n",
        "    src_by_name = {l.name: l for l in src_model.layers}\n",
        "    for tgt in model_320.layers:\n",
        "        if tgt.name in src_by_name:\n",
        "            src_w = src_by_name[tgt.name].get_weights()\n",
        "            if not src_w:\n",
        "                continue\n",
        "            try:\n",
        "                tgt.set_weights(src_w)\n",
        "                copied += 1\n",
        "            except Exception:\n",
        "                # shape mismatch for this layer — skip\n",
        "                continue\n",
        "    print(f\"Copied weights for {copied} layers (best-effort).\")\n",
        "\n",
        "# ---------- unfreeze top N ----------\n",
        "total_layers = len(model_320.layers)\n",
        "start_unfreeze = max(0, total_layers - UNFREEZE_TOP_N)\n",
        "for i, layer in enumerate(model_320.layers):\n",
        "    layer.trainable = True if i >= start_unfreeze else False\n",
        "print(f\"Unfrozen layers from {start_unfreeze}/{total_layers-1} (total layers: {total_layers})\")\n",
        "\n",
        "# ---------- compile ----------\n",
        "opt = Adam(learning_rate=LR_RESIZE)\n",
        "loss = CategoricalCrossentropy(label_smoothing=0.05)\n",
        "model_320.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
        "model_320.summary()\n",
        "\n",
        "# ---------- callbacks ----------\n",
        "ckpt = ModelCheckpoint(str(MODEL_DIR / \"best_resize.keras\"), monitor=\"val_accuracy\", save_best_only=True, verbose=1)\n",
        "reducelr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "early = EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True, verbose=1)\n",
        "\n",
        "# ---------- train ----------\n",
        "history_resize = model_320.fit(\n",
        "    train_ds_320,\n",
        "    validation_data=val_ds_320,\n",
        "    epochs=EPOCHS_RESIZE,\n",
        "    callbacks=[ckpt, reducelr, early],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ---------- save best & evaluate ----------\n",
        "best_resize = MODEL_DIR / \"best_resize.keras\"\n",
        "if best_resize.exists():\n",
        "    final_model = tf.keras.models.load_model(str(best_resize))\n",
        "    print(\"Loaded best resize model:\", best_resize)\n",
        "else:\n",
        "    final_model = model_320\n",
        "\n",
        "final_model.save(str(FINAL_RESIZED), include_optimizer=False)\n",
        "print(\"Saved final resized model ->\", FINAL_RESIZED)\n",
        "\n",
        "# evaluate on test set\n",
        "y_true, y_pred = [], []\n",
        "for imgs, labs in test_ds_320:\n",
        "    probs = final_model.predict(imgs, verbose=0)\n",
        "    y_true.extend(np.argmax(labs.numpy(), axis=1).tolist())\n",
        "    y_pred.extend(np.argmax(probs, axis=1).tolist())\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(f\"\\nFinal test accuracy (320×320): {acc:.4f}\")\n",
        "print(classification_report(y_true, y_pred, digits=4))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-24T18:19:20.372888Z",
          "iopub.execute_input": "2025-09-24T18:19:20.37316Z",
          "iopub.status.idle": "2025-09-24T19:04:47.741951Z",
          "shell.execute_reply.started": "2025-09-24T18:19:20.373141Z",
          "shell.execute_reply": "2025-09-24T19:04:47.741184Z"
        },
        "id": "kSOMSnV0TbLh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrected: TTA + simple ensemble of best_phaseB + final_model_320\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# config\n",
        "IMG = (320, 320)\n",
        "BATCH = 24\n",
        "SRC_TEST = str(Path(PROCESSED_ROOT) / \"test\")\n",
        "m1_path = MODEL_DIR / \"best_phaseB.keras\"\n",
        "m2_path = MODEL_DIR / \"final_model_320.keras\"   # may exist after resize\n",
        "models_to_use = []\n",
        "if m1_path.exists():\n",
        "    models_to_use.append(tf.keras.models.load_model(str(m1_path)))\n",
        "if m2_path.exists():\n",
        "    models_to_use.append(tf.keras.models.load_model(str(m2_path)))\n",
        "if not models_to_use:\n",
        "    raise FileNotFoundError(\"No candidate models found (best_phaseB/final_model_320).\")\n",
        "\n",
        "print(\"Using models:\", [getattr(m, \"name\", \"model\") for m in models_to_use])\n",
        "\n",
        "# build test dataset (320)\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    SRC_TEST, labels='inferred', label_mode='categorical',\n",
        "    image_size=IMG, batch_size=BATCH, shuffle=False, seed=777\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "def tta_variants(batch_images):\n",
        "    orig = tf.cast(batch_images, tf.float32)\n",
        "    flip = tf.image.flip_left_right(orig)\n",
        "    crop = tf.image.central_crop(orig, 0.92)\n",
        "    crop = tf.image.resize(crop, IMG)\n",
        "    cont = tf.image.adjust_contrast(orig, 1.08)\n",
        "    variants = tf.concat([orig, flip, crop, cont], axis=0)  # (4*B, H, W, 3)\n",
        "    return variants, 4\n",
        "\n",
        "y_true_all = []\n",
        "y_pred_all = []\n",
        "for imgs, labs in test_ds:\n",
        "    batch_size = imgs.shape[0]\n",
        "    variants, nvar = tta_variants(imgs)  # shape (nvar*batch, H, W, 3)\n",
        "\n",
        "    model_preds = []\n",
        "    for m in models_to_use:\n",
        "        preds = m.predict(variants, verbose=0)           # (nvar*batch, C)\n",
        "        preds = preds.reshape(nvar, batch_size, -1)      # (nvar, batch, C)\n",
        "        preds = preds.mean(axis=0)                       # (batch, C)\n",
        "        model_preds.append(preds)\n",
        "\n",
        "    ensemble_preds = np.mean(np.stack(model_preds, axis=0), axis=0)  # (batch, C)\n",
        "    y_true_all.extend(np.argmax(labs.numpy(), axis=1).tolist())\n",
        "    y_pred_all.extend(np.argmax(ensemble_preds, axis=1).tolist())\n",
        "\n",
        "acc = accuracy_score(y_true_all, y_pred_all)\n",
        "print(f\"\\nEnsembled TTA Test accuracy: {acc:.4f}\")\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_true_all, y_pred_all, digits=4))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-24T19:09:06.95993Z",
          "iopub.execute_input": "2025-09-24T19:09:06.96072Z",
          "iopub.status.idle": "2025-09-24T19:09:59.403207Z",
          "shell.execute_reply.started": "2025-09-24T19:09:06.960695Z",
          "shell.execute_reply": "2025-09-24T19:09:59.402461Z"
        },
        "id": "633odXomTbLh"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
